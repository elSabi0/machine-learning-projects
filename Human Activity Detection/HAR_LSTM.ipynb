{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activities are the class labels\n",
    "# It is a 6 class classification\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATADIR = 'UCI_HAR_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data signals\n",
    "# Signals are from Accelerometer and Gyroscope\n",
    "# The signals are in x,y,z directions\n",
    "# Sensor signals are filtered to have only body acceleration\n",
    "# excluding the acceleration due to gravity\n",
    "# Triaxial acceleration from the accelerometer is total acceleration\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"total_acc_x\",\n",
    "    \"total_acc_y\",\n",
    "    \"total_acc_z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to read the data from csv file\n",
    "def _read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "# Utility function to load the load\n",
    "def load_signals(subset):\n",
    "    signals_data = []\n",
    "\n",
    "    for signal in SIGNALS:\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "        signals_data.append(\n",
    "            _read_csv(filename).as_matrix()\n",
    "        ) \n",
    "\n",
    "    # Transpose is used to change the dimensionality of the output,\n",
    "    # aggregating the signals by combination of sample/timestep.\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    return np.transpose(signals_data, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_y(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "\n",
    "    return pd.get_dummies(y).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train, y_test = load_y('train'), load_y('test')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Importing tensorflow\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring a session\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Keras\n",
    "from keras import backend as K\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "\"\"\"\n",
    "epochs = 30\n",
    "batch_size = 16                  #this configuration is not improving our accuracy\n",
    "n_hidden = 64                     accuracy is around 90.\n",
    "drop_out = 0.5\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 16                  # this is also not working well for us; accuracy 91.45 ~92%\n",
    "n_hidden = 64\n",
    "drop_out = 0.25\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 32                  #this configuration seems to me as it's doing overfitting\n",
    "n_hidden = 64                     accuracy increase to a certain point and then it decreases\n",
    "drop_out = 0.25                     \n",
    "\n",
    "\n",
    "\n",
    "epochs = 30                      # 92.4% \n",
    "batch_size = 64               \n",
    "n_hidden = 256               \n",
    "drop_out = 0.5\n",
    "\n",
    "\n",
    "epochs = 30                     # performance decreases after 25/26 epochs\n",
    "batch_size = 64                  accuracy = 88.6 whereas around 25/26 epchs accuracy was ~93%\n",
    "n_hidden = 256                  \n",
    "drop_out = 0.75\n",
    "\n",
    "\n",
    "epochs = 35                     # 90.77\n",
    "batch_size = 64                  \n",
    "n_hidden = 256                  \n",
    "drop_out = 0.6\n",
    "\n",
    "\n",
    "epochs = 30                     # 92.23%\n",
    "batch_size = 32                  \n",
    "n_hidden = 256                  \n",
    "drop_out = 0.6\n",
    "\"\"\"\n",
    "\n",
    "epochs = 35                     # 92.23%\n",
    "batch_size = 32                  \n",
    "n_hidden = 256                  \n",
    "drop_out = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to count the number of classes\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train and test data\n",
    "X_train, X_test, Y_train, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "7352\n"
     ]
    }
   ],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defining the Architecture of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0801 15:00:43.155395  5604 nn_ops.py:4224] Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 256)               272384    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 273,926\n",
      "Trainable params: 273,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initiliazing the sequential model\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(drop_out))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/35\n",
      "7352/7352 [==============================] - 112s 15ms/step - loss: 1.3776 - acc: 0.3965 - val_loss: 1.3157 - val_acc: 0.4018\n",
      "Epoch 2/35\n",
      "7352/7352 [==============================] - 114s 16ms/step - loss: 1.1997 - acc: 0.4884 - val_loss: 1.1784 - val_acc: 0.4225\n",
      "Epoch 3/35\n",
      "7352/7352 [==============================] - 109s 15ms/step - loss: 1.2064 - acc: 0.4656 - val_loss: 1.2801 - val_acc: 0.3960\n",
      "Epoch 4/35\n",
      "7352/7352 [==============================] - 109s 15ms/step - loss: 0.9821 - acc: 0.5619 - val_loss: 0.9414 - val_acc: 0.5803\n",
      "Epoch 5/35\n",
      "7352/7352 [==============================] - 111s 15ms/step - loss: 0.7248 - acc: 0.6802 - val_loss: 0.6545 - val_acc: 0.7180\n",
      "Epoch 6/35\n",
      "7352/7352 [==============================] - 110s 15ms/step - loss: 0.5563 - acc: 0.7916 - val_loss: 0.4767 - val_acc: 0.8497\n",
      "Epoch 7/35\n",
      "7352/7352 [==============================] - 111s 15ms/step - loss: 0.3894 - acc: 0.8686 - val_loss: 0.3285 - val_acc: 0.8935\n",
      "Epoch 8/35\n",
      "7352/7352 [==============================] - 110s 15ms/step - loss: 0.3135 - acc: 0.8970 - val_loss: 0.3488 - val_acc: 0.8843\n",
      "Epoch 9/35\n",
      "7352/7352 [==============================] - 109s 15ms/step - loss: 0.2259 - acc: 0.9202 - val_loss: 0.2961 - val_acc: 0.8999\n",
      "Epoch 10/35\n",
      "7352/7352 [==============================] - 111s 15ms/step - loss: 0.2085 - acc: 0.9283 - val_loss: 0.3688 - val_acc: 0.8928\n",
      "Epoch 11/35\n",
      "7352/7352 [==============================] - 114s 15ms/step - loss: 0.1785 - acc: 0.9342 - val_loss: 0.3355 - val_acc: 0.9036\n",
      "Epoch 12/35\n",
      "7352/7352 [==============================] - 111s 15ms/step - loss: 0.2028 - acc: 0.9353 - val_loss: 0.3333 - val_acc: 0.9006\n",
      "Epoch 13/35\n",
      "7352/7352 [==============================] - 110s 15ms/step - loss: 0.2001 - acc: 0.9342 - val_loss: 0.3599 - val_acc: 0.9070\n",
      "Epoch 14/35\n",
      "7352/7352 [==============================] - 111s 15ms/step - loss: 0.1668 - acc: 0.9400 - val_loss: 0.3746 - val_acc: 0.8812\n",
      "Epoch 15/35\n",
      "7352/7352 [==============================] - 107s 15ms/step - loss: 0.1632 - acc: 0.9407 - val_loss: 0.2589 - val_acc: 0.9104\n",
      "Epoch 16/35\n",
      "7352/7352 [==============================] - 108s 15ms/step - loss: 0.1521 - acc: 0.9419 - val_loss: 0.3209 - val_acc: 0.9131\n",
      "Epoch 17/35\n",
      "7352/7352 [==============================] - 110s 15ms/step - loss: 0.1441 - acc: 0.9450 - val_loss: 0.5828 - val_acc: 0.8836\n",
      "Epoch 18/35\n",
      "7352/7352 [==============================] - 109s 15ms/step - loss: 0.1608 - acc: 0.9450 - val_loss: 0.5610 - val_acc: 0.9057\n",
      "Epoch 19/35\n",
      "7352/7352 [==============================] - 112s 15ms/step - loss: 0.1478 - acc: 0.9440 - val_loss: 0.3182 - val_acc: 0.9270\n",
      "Epoch 20/35\n",
      "7352/7352 [==============================] - 117s 16ms/step - loss: 0.1352 - acc: 0.9480 - val_loss: 0.4048 - val_acc: 0.9155\n",
      "Epoch 21/35\n",
      "7352/7352 [==============================] - 114s 15ms/step - loss: 0.1427 - acc: 0.9476 - val_loss: 0.9192 - val_acc: 0.8741\n",
      "Epoch 22/35\n",
      "7352/7352 [==============================] - 111s 15ms/step - loss: 0.1457 - acc: 0.9518 - val_loss: 0.6207 - val_acc: 0.8914\n",
      "Epoch 23/35\n",
      "7352/7352 [==============================] - 112s 15ms/step - loss: 0.1599 - acc: 0.9467 - val_loss: 0.4843 - val_acc: 0.8941\n",
      "Epoch 24/35\n",
      "7352/7352 [==============================] - 114s 16ms/step - loss: 0.1375 - acc: 0.9449 - val_loss: 0.4970 - val_acc: 0.9016\n",
      "Epoch 25/35\n",
      "7352/7352 [==============================] - 114s 15ms/step - loss: 0.1583 - acc: 0.9489 - val_loss: 0.3234 - val_acc: 0.9172\n",
      "Epoch 26/35\n",
      "7352/7352 [==============================] - 111s 15ms/step - loss: 0.1288 - acc: 0.9512 - val_loss: 0.4670 - val_acc: 0.9169\n",
      "Epoch 27/35\n",
      "7352/7352 [==============================] - 110s 15ms/step - loss: 0.3127 - acc: 0.9233 - val_loss: 0.5671 - val_acc: 0.8951\n",
      "Epoch 28/35\n",
      "7352/7352 [==============================] - 110s 15ms/step - loss: 0.1567 - acc: 0.9403 - val_loss: 0.4284 - val_acc: 0.9108\n",
      "Epoch 29/35\n",
      "7352/7352 [==============================] - 109s 15ms/step - loss: 0.1582 - acc: 0.9399 - val_loss: 0.4918 - val_acc: 0.8985\n",
      "Epoch 30/35\n",
      "7352/7352 [==============================] - 110s 15ms/step - loss: 0.1421 - acc: 0.9440 - val_loss: 0.4605 - val_acc: 0.9006\n",
      "Epoch 31/35\n",
      "7352/7352 [==============================] - 110s 15ms/step - loss: 0.1406 - acc: 0.9457 - val_loss: 0.4002 - val_acc: 0.9158\n",
      "Epoch 32/35\n",
      "7352/7352 [==============================] - 110s 15ms/step - loss: 0.1325 - acc: 0.9490 - val_loss: 0.4676 - val_acc: 0.9077\n",
      "Epoch 33/35\n",
      "7352/7352 [==============================] - 113s 15ms/step - loss: 0.1428 - acc: 0.9459 - val_loss: 0.3987 - val_acc: 0.9087\n",
      "Epoch 34/35\n",
      "7352/7352 [==============================] - 116s 16ms/step - loss: 0.1276 - acc: 0.9486 - val_loss: 0.5826 - val_acc: 0.9111\n",
      "Epoch 35/35\n",
      "7352/7352 [==============================] - 116s 16ms/step - loss: 0.1388 - acc: 0.9508 - val_loss: 0.4792 - val_acc: 0.9019\n",
      "Time taken :  1:04:53.814918\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "\n",
    "print(\"Time taken : \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 1s 461us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.39372028857254565, 0.9019341703427214]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With a simple 2 layer architecture we got 90.09% accuracy and a loss of 0.30\n",
    "- We can further imporve the performace with Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model with 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"                        this configuration gives accuracy of 90.84\n",
    "epochs_m2 = 30\n",
    "batch_size_m2= 32\n",
    "n_hidden_layer1 = 128\n",
    "n_hidden_layer2 =32\n",
    "drop_out_1 = 0.2\n",
    "drop_out_2 = 0.5\n",
    "\n",
    "\n",
    "epochs_m2 = 30                    92.16\n",
    "batch_size_m2= 32\n",
    "n_hidden_layer1 = 128\n",
    "n_hidden_layer2 =64\n",
    "drop_out_1 = 0.2\n",
    "drop_out_2 = 0.5\n",
    "\n",
    "\n",
    "\n",
    "epochs_m2 = 30                   # 89.89\n",
    "batch_size_m2= 8\n",
    "n_hidden_layer1 = 32\n",
    "n_hidden_layer2 =64\n",
    "drop_out_1 = 0.5\n",
    "drop_out_2 = 0.5\n",
    "\n",
    "\n",
    "epochs_m2 = 30                   # 90.19\n",
    "batch_size_m2= 64\n",
    "n_hidden_layer1 = 32\n",
    "n_hidden_layer2 =64\n",
    "drop_out_1 = 0.5\n",
    "drop_out_2 = 0.5\n",
    "\n",
    "\n",
    "epochs_m2 = 60                   # 91.99\n",
    "batch_size_m2= 64\n",
    "n_hidden_layer1 = 32\n",
    "n_hidden_layer2 =64\n",
    "drop_out_1 = 0.5\n",
    "drop_out_2 = 0.5\n",
    "\n",
    "\n",
    "epochs_m2 = 120              # 91.99\n",
    "batch_size_m2= 64\n",
    "n_hidden_layer1 = 32\n",
    "n_hidden_layer2 =64\n",
    "drop_out_1 = 0.5\n",
    "drop_out_2 = 0.5\n",
    "\"\"\"\n",
    "\n",
    "epochs_m2 = 50                  #92.16\n",
    "batch_size_m2= 64\n",
    "n_hidden_layer1 = 32\n",
    "n_hidden_layer2 =64\n",
    "drop_out_1 = 0.5\n",
    "drop_out_2 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, 128, 32)           5376      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 30,598\n",
      "Trainable params: 30,598\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.normalization import BatchNormalization as BNorm\n",
    "\n",
    "# Initiliazing the sequential model\n",
    "model2 = Sequential()\n",
    "# Configuring the parameters\n",
    "model2.add(LSTM(n_hidden_layer1, return_sequences=True, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model2.add(Dropout(drop_out_1))\n",
    "# Adding batch normalization\n",
    "model.add(BNorm())\n",
    "\n",
    "model2.add(LSTM(n_hidden_layer2))\n",
    "# Adding a dropout layer\n",
    "model2.add(Dropout(drop_out_2))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model2.add(Dense(n_classes, activation='sigmoid'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/50\n",
      "7352/7352 [==============================] - 42s 6ms/step - loss: 1.3379 - acc: 0.4052 - val_loss: 1.1464 - val_acc: 0.4808\n",
      "Epoch 2/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 1.0095 - acc: 0.5600 - val_loss: 0.9177 - val_acc: 0.5589\n",
      "Epoch 3/50\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.8061 - acc: 0.6287 - val_loss: 0.8159 - val_acc: 0.5901\n",
      "Epoch 4/50\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.7390 - acc: 0.6341 - val_loss: 0.7766 - val_acc: 0.6138\n",
      "Epoch 5/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.6924 - acc: 0.6473 - val_loss: 0.7360 - val_acc: 0.6071\n",
      "Epoch 6/50\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.6916 - acc: 0.6658 - val_loss: 0.7560 - val_acc: 0.6281\n",
      "Epoch 7/50\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.6484 - acc: 0.6990 - val_loss: 0.8120 - val_acc: 0.6783\n",
      "Epoch 8/50\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.5634 - acc: 0.7444 - val_loss: 0.6664 - val_acc: 0.7136\n",
      "Epoch 9/50\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 0.5178 - acc: 0.7688 - val_loss: 0.6337 - val_acc: 0.7350\n",
      "Epoch 10/50\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.4684 - acc: 0.7821 - val_loss: 0.5861 - val_acc: 0.7316\n",
      "Epoch 11/50\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.4235 - acc: 0.8056 - val_loss: 0.6806 - val_acc: 0.7218\n",
      "Epoch 12/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.4001 - acc: 0.8275 - val_loss: 0.5652 - val_acc: 0.8181\n",
      "Epoch 13/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.3605 - acc: 0.8674 - val_loss: 0.5227 - val_acc: 0.8521\n",
      "Epoch 14/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.3465 - acc: 0.8806 - val_loss: 0.4897 - val_acc: 0.8619\n",
      "Epoch 15/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.3008 - acc: 0.9101 - val_loss: 0.4340 - val_acc: 0.8731\n",
      "Epoch 16/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.2215 - acc: 0.9276 - val_loss: 0.5210 - val_acc: 0.8565\n",
      "Epoch 17/50\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2091 - acc: 0.9314 - val_loss: 0.4617 - val_acc: 0.8677\n",
      "Epoch 18/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1975 - acc: 0.9350 - val_loss: 0.5060 - val_acc: 0.8537\n",
      "Epoch 19/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1708 - acc: 0.9388 - val_loss: 0.5711 - val_acc: 0.8622\n",
      "Epoch 20/50\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1664 - acc: 0.9425 - val_loss: 0.3850 - val_acc: 0.8989\n",
      "Epoch 21/50\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1653 - acc: 0.9408 - val_loss: 0.3810 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1652 - acc: 0.9427 - val_loss: 0.3858 - val_acc: 0.9030\n",
      "Epoch 23/50\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1475 - acc: 0.9430 - val_loss: 0.5183 - val_acc: 0.8707\n",
      "Epoch 24/50\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1360 - acc: 0.9498 - val_loss: 0.3979 - val_acc: 0.9023\n",
      "Epoch 25/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1393 - acc: 0.9464 - val_loss: 0.8113 - val_acc: 0.8239\n",
      "Epoch 26/50\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1510 - acc: 0.9449 - val_loss: 0.4087 - val_acc: 0.8884\n",
      "Epoch 27/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1473 - acc: 0.9471 - val_loss: 0.3735 - val_acc: 0.9114\n",
      "Epoch 28/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1324 - acc: 0.9508 - val_loss: 0.3661 - val_acc: 0.8965\n",
      "Epoch 29/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1462 - acc: 0.9472 - val_loss: 0.3547 - val_acc: 0.9080\n",
      "Epoch 30/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1347 - acc: 0.9482 - val_loss: 0.3525 - val_acc: 0.9131\n",
      "Epoch 31/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1238 - acc: 0.9517 - val_loss: 0.3504 - val_acc: 0.9138\n",
      "Epoch 32/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1316 - acc: 0.9524 - val_loss: 0.2904 - val_acc: 0.9111\n",
      "Epoch 33/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1311 - acc: 0.9502 - val_loss: 0.3656 - val_acc: 0.9152\n",
      "Epoch 34/50\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1201 - acc: 0.9501 - val_loss: 0.4041 - val_acc: 0.8979\n",
      "Epoch 35/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1369 - acc: 0.9509 - val_loss: 0.4479 - val_acc: 0.9033\n",
      "Epoch 36/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1246 - acc: 0.9536 - val_loss: 0.3590 - val_acc: 0.9053\n",
      "Epoch 37/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1235 - acc: 0.9532 - val_loss: 0.3657 - val_acc: 0.9162\n",
      "Epoch 38/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1271 - acc: 0.9521 - val_loss: 0.3441 - val_acc: 0.9111\n",
      "Epoch 39/50\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1193 - acc: 0.9553 - val_loss: 0.3912 - val_acc: 0.9104\n",
      "Epoch 40/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1156 - acc: 0.9565 - val_loss: 0.3588 - val_acc: 0.9046\n",
      "Epoch 41/50\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1182 - acc: 0.9536 - val_loss: 0.3727 - val_acc: 0.9091\n",
      "Epoch 42/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1156 - acc: 0.9540 - val_loss: 0.4680 - val_acc: 0.9023\n",
      "Epoch 43/50\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1187 - acc: 0.9547 - val_loss: 0.4085 - val_acc: 0.9060\n",
      "Epoch 44/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1226 - acc: 0.9544 - val_loss: 0.3699 - val_acc: 0.9080\n",
      "Epoch 45/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1098 - acc: 0.9554 - val_loss: 0.4554 - val_acc: 0.9057\n",
      "Epoch 46/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1119 - acc: 0.9565 - val_loss: 0.4115 - val_acc: 0.9111\n",
      "Epoch 47/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1158 - acc: 0.9546 - val_loss: 0.3713 - val_acc: 0.9111\n",
      "Epoch 48/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1090 - acc: 0.9558 - val_loss: 0.4388 - val_acc: 0.9016\n",
      "Epoch 49/50\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1472 - acc: 0.9531 - val_loss: 0.5271 - val_acc: 0.9016\n",
      "Epoch 50/50\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1287 - acc: 0.9536 - val_loss: 0.4692 - val_acc: 0.9060\n",
      "Time taken :  0:30:43.677781\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# Training the model\n",
    "model2.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size_m2,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs_m2)\n",
    "\n",
    "print(\"Time taken : \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 7s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model2.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan, 0.168306752629793]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In classic machine learning models we got accuracy of around 96% and here we also tried to get the accuracy close to 96% by\n",
    "* tuning the number of lstm units\n",
    "* experimenting with drop out value \n",
    "* and by also adding a secnd hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For our first model with single hidden layer I tried various configurations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+------------+--------------+---------+----------+\n",
      "| Configuration | Epochs | Batch Size | Hidden Layer | Dropout | Accuracy |\n",
      "+---------------+--------+------------+--------------+---------+----------+\n",
      "|       1       |   30   |     16     |      64      |   0.25  |   90.6   |\n",
      "|       2       |   30   |     16     |      64      |   0.25  |  91.45   |\n",
      "|       3       |   30   |     32     |      64      |   0.25  |  90.84   |\n",
      "|       4       |   30   |     32     |     128      |   0.5   |  92.81   |\n",
      "+---------------+--------+------------+--------------+---------+----------+\n"
     ]
    }
   ],
   "source": [
    "##################################### pretty table for model 1 with 1 lstm layer ####################################\n",
    "number       = [1, 2, 3, 4]\n",
    "epochs       = [30, 30, 30, 30]\n",
    "batch_size   = [16, 16, 32, 32]\n",
    "n_hidden     = [64, 64, 64, 128]\n",
    "drop_out     = [0.25, 0.25, 0.25, 0.5]\n",
    "accuracy     = [90.6, 91.45, 90.84, 92.81]\n",
    "\n",
    "# Initializing prettytable # Adding columns \n",
    "ptable = PrettyTable()\n",
    "ptable.add_column(\"Configuration\",number)\n",
    "ptable.add_column(\"Epochs\", epochs)\n",
    "ptable.add_column(\"Batch Size\",batch_size)\n",
    "ptable.add_column(\"Hidden Layer\",n_hidden) \n",
    "ptable.add_column(\"Dropout\",drop_out) \n",
    "ptable.add_column(\"Accuracy\",accuracy) \n",
    "#Printing the Table\n",
    "print(ptable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+------------+--------------+--------------+---------+---------+----------+\n",
      "| Configuration | Epochs | Batch Size | Hidden Layer | Hidden Layer | Dropout | Dropout | Accuracy |\n",
      "+---------------+--------+------------+--------------+--------------+---------+---------+----------+\n",
      "|       1       |   30   |     32     |     128      |      32      |   0.2   |   0.2   |  90.84   |\n",
      "|       2       |   30   |     32     |     128      |      64      |   0.5   |   0.5   |  92.16   |\n",
      "+---------------+--------+------------+--------------+--------------+---------+---------+----------+\n"
     ]
    }
   ],
   "source": [
    "################################ pretty table for model 2 with 2 lstm layesr ##############################\n",
    "number          = [1, 2]\n",
    "epochs          = [30, 30]\n",
    "batch_size      = [32, 32]\n",
    "n_hidden_layer1 = [128, 128]\n",
    "n_hidden_layer2 = [32, 64]\n",
    "drop_out_1      = [0.2, 0.5]\n",
    "drop_out_2      = [0.2, 0.5]\n",
    "accuracy     = [90.84, 92.16]\n",
    "\n",
    "# Initializing prettytable # Adding columns \n",
    "ptable = PrettyTable()\n",
    "ptable.add_column(\"Configuration\",number)\n",
    "ptable.add_column(\"Epochs\", epochs)\n",
    "ptable.add_column(\"Batch Size\",batch_size)\n",
    "ptable.add_column(\"Hidden Layer\",n_hidden_layer1)\n",
    "ptable.add_column(\"Hidden Layer\",n_hidden_layer2)\n",
    "ptable.add_column(\"Dropout\",drop_out_1)\n",
    "ptable.add_column(\"Dropout\",drop_out_2)\n",
    "ptable.add_column(\"Accuracy\",accuracy) \n",
    "#Printing the Table\n",
    "print(ptable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: One observation which I made while training our lstm models is that after training the model more than one time we get differnt values of accuracy i.e for same configuration we might get different accuracy values every time we train it over again.\n",
    "\n",
    "\n",
    "So the results may improve if we train these models again with same configuration.\n",
    "The final configurations for each model i.e model 1 and model 2 are good and we can get accuracy closer to 96%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
